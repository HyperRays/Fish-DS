{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a6211c",
   "metadata": {},
   "source": [
    "# FISH-DS Interactive Visualization with Rerun\n",
    "\n",
    "This notebook provides an advanced 3D interactive visualization of FISH-DS magnetohydrodynamics (MHD) simulation results using [Rerun](https://rerun.io/). Rerun offers real-time 3D visualization with interactive controls, timeline scrubbing, and multi-dimensional data exploration.\n",
    "\n",
    "## Features\n",
    "- **3D Volume Rendering**: Interactive 3D visualization of scalar and vector fields\n",
    "- **Time Series Animation**: Scrub through simulation timesteps\n",
    "- **Multi-Variable Display**: Simultaneous visualization of density, velocity, magnetic fields\n",
    "- **Vector Field Visualization**: 3D arrows for velocity and magnetic field vectors\n",
    "- **Interactive Controls**: Real-time parameter adjustment and view manipulation\n",
    "- **Cross-Platform**: Works in Jupyter notebooks and standalone viewer\n",
    "\n",
    "## Data Structure\n",
    "- **Input**: FISH-DS binary `.dat` files from simulation output\n",
    "- **Variables**: Velocity vectors, magnetic fields, density, entropy/pressure\n",
    "- **Geometry**: 3D grid data with multiple plane segments (XY, XZ, YZ)\n",
    "- **Temporal**: Multiple timesteps for animation and time series analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e53b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import struct\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Rerun for 3D visualization\n",
    "try:\n",
    "    import rerun as rr\n",
    "    print(\"‚úÖ Rerun imported successfully!\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Rerun not found. Installing...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"rerun-sdk\"])\n",
    "    import rerun as rr\n",
    "    print(\"‚úÖ Rerun installed and imported!\")\n",
    "\n",
    "# Additional libraries for data processing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.cm import get_cmap\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output\n",
    "    print(\"‚úÖ Jupyter widgets available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Jupyter widgets not available - using basic controls\")\n",
    "\n",
    "print(\"üöÄ All libraries imported successfully!\")\n",
    "print(f\"üìä Rerun version: {rr.__version__}\")\n",
    "\n",
    "# Initialize Rerun\n",
    "rr.init(\"FISH-DS-MHD-Simulation\", spawn=True)\n",
    "print(\"üéÆ Rerun viewer initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c7c10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FISH-DS Data Reading Functions\n",
    "# Adapted from the original vis.ipynb notebook\n",
    "\n",
    "def read_fortran_record(file_handle, dtype, count=1):\n",
    "    \"\"\"\n",
    "    Read a Fortran unformatted record.\n",
    "    Fortran unformatted files have record markers before and after each record.\n",
    "    \"\"\"\n",
    "    # Read record size (4 bytes)\n",
    "    record_size = struct.unpack('i', file_handle.read(4))[0]\n",
    "    \n",
    "    # Read the actual data\n",
    "    if dtype == 'char':\n",
    "        data = file_handle.read(count).decode('ascii', errors='ignore')\n",
    "    elif dtype == 'float':\n",
    "        data = struct.unpack(f'{count}f', file_handle.read(4 * count))\n",
    "        if count == 1:\n",
    "            data = data[0]\n",
    "    elif dtype == 'int':\n",
    "        data = struct.unpack(f'{count}i', file_handle.read(4 * count))\n",
    "        if count == 1:\n",
    "            data = data[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dtype: {dtype}\")\n",
    "    \n",
    "    # Read trailing record size\n",
    "    trailing_size = struct.unpack('i', file_handle.read(4))[0]\n",
    "    \n",
    "    if record_size != trailing_size:\n",
    "        print(f\"Warning: Record size mismatch: {record_size} != {trailing_size}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def read_fish_data(filename: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Read FISH-DS binary output file and return structured data.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    data : dict\n",
    "        Dictionary containing simulation data with keys:\n",
    "        - 'date': simulation date\n",
    "        - 'time': simulation time\n",
    "        - 'dx': grid spacing\n",
    "        - 'segments': list of data segments (planes)\n",
    "        - 'domain_overview': reduced resolution domain data\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(f\"File not found: {filename}\")\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    with open(filename, 'rb') as f:\n",
    "        # Read header information\n",
    "        data['date'] = read_fortran_record(f, 'char', 8).strip()\n",
    "        data['time'], data['dx'] = read_fortran_record(f, 'float', 2)\n",
    "        ns, num, mr = read_fortran_record(f, 'int', 3)\n",
    "        \n",
    "        data['num_segments'] = ns\n",
    "        data['time_step'] = num\n",
    "        data['processor_rank'] = mr\n",
    "        \n",
    "        print(f\"üìÖ Date: {data['date']}\")\n",
    "        print(f\"‚è∞ Time: {data['time']:.6f}\")\n",
    "        print(f\"üìè Grid spacing: {data['dx']:.6f}\")\n",
    "        print(f\"üìä Segments: {ns}, Time step: {num}, Processor: {mr}\")\n",
    "        \n",
    "        # Read data segments (3 planes: xy, xz, yz)\n",
    "        data['segments'] = []\n",
    "        \n",
    "        for seg_idx in range(ns):\n",
    "            segment = {}\n",
    "            \n",
    "            # Read segment header\n",
    "            nv_total = read_fortran_record(f, 'int', 1)\n",
    "            bounds = read_fortran_record(f, 'int', 6)\n",
    "            \n",
    "            segment['num_variables'] = nv_total\n",
    "            segment['bounds'] = bounds\n",
    "            imin, imax, jmin, jmax, kmin, kmax = bounds\n",
    "            \n",
    "            nx = max(imax - imin + 1, 0)\n",
    "            ny = max(jmax - jmin + 1, 0)\n",
    "            nz = max(kmax - kmin + 1, 0)\n",
    "            \n",
    "            segment['dimensions'] = (nx, ny, nz)\n",
    "            \n",
    "            print(f\"  üìê Segment {seg_idx + 1}: {nx}√ó{ny}√ó{nz}, {nv_total} variables\")\n",
    "            \n",
    "            # Read segment data\n",
    "            if nx * ny * nz > 0:\n",
    "                total_points = nx * ny * nz * nv_total\n",
    "                raw_data = read_fortran_record(f, 'float', total_points)\n",
    "                \n",
    "                # Reshape data: variables are interleaved for each spatial point\n",
    "                segment['raw_data'] = np.array(raw_data).reshape((nz, ny, nx, nv_total))\n",
    "                \n",
    "                if nv_total >= 6:\n",
    "                    # Extract velocity components (momentum and magnetic field)\n",
    "                    segment['velocity'] = segment['raw_data'][:, :, :, :6]\n",
    "                if nv_total >= 8:\n",
    "                    # Extract scalar fields (density, entropy)\n",
    "                    segment['scalars'] = segment['raw_data'][:, :, :, 6:8]\n",
    "                \n",
    "            data['segments'].append(segment)\n",
    "        \n",
    "        # Try to read domain overview if present\n",
    "        try:\n",
    "            nred, i0, j0, k0 = read_fortran_record(f, 'int', 4)\n",
    "            nv_total = read_fortran_record(f, 'int', 1)\n",
    "            bounds = read_fortran_record(f, 'int', 6)\n",
    "            \n",
    "            imin, imax, jmin, jmax, kmin, kmax = bounds\n",
    "            nx = max(imax - imin + 1, 0)\n",
    "            ny = max(jmax - jmin + 1, 0)\n",
    "            nz = max(kmax - kmin + 1, 0)\n",
    "            \n",
    "            if nx * ny * nz > 0:\n",
    "                total_points = nx * ny * nz * nv_total\n",
    "                raw_data = read_fortran_record(f, 'float', total_points)\n",
    "                \n",
    "                domain_overview = {\n",
    "                    'reduction_factor': nred,\n",
    "                    'offset': (i0, j0, k0),\n",
    "                    'dimensions': (nx, ny, nz),\n",
    "                    'num_variables': nv_total,\n",
    "                    'bounds': bounds,\n",
    "                    'raw_data': np.array(raw_data).reshape((nz, ny, nx, nv_total))\n",
    "                }\n",
    "                \n",
    "                if nv_total >= 6:\n",
    "                    domain_overview['velocity'] = domain_overview['raw_data'][:, :, :, :6]\n",
    "                if nv_total >= 8:\n",
    "                    domain_overview['scalars'] = domain_overview['raw_data'][:, :, :, 6:8]\n",
    "                \n",
    "                data['domain_overview'] = domain_overview\n",
    "                print(f\"  üåê Domain overview: {nx}√ó{ny}√ó{nz}, reduction factor: {nred}\")\n",
    "                \n",
    "        except:\n",
    "            print(\"  ‚ö†Ô∏è  No domain overview data found\")\n",
    "            data['domain_overview'] = None\n",
    "    \n",
    "    return data\n",
    "\n",
    "def parse_mhd_variables(data_segment: Dict[str, Any]) -> Optional[Dict[str, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Parse MHD variables from raw data segment.\n",
    "    \n",
    "    According to FISH-DS structure:\n",
    "    - Variables 0-2: momentum density components (vx, vy, vz) * density\n",
    "    - Variables 3-5: magnetic field components (Bx, By, Bz) / sqrt(4œÄ)\n",
    "    - Variable 6: density \n",
    "    - Variable 7: entropy/internal energy density\n",
    "    \"\"\"\n",
    "    if 'raw_data' not in data_segment:\n",
    "        return None\n",
    "    \n",
    "    raw = data_segment['raw_data']\n",
    "    nz, ny, nx, nvar = raw.shape\n",
    "    \n",
    "    variables = {}\n",
    "    \n",
    "    if nvar >= 6:\n",
    "        # Velocity/momentum components\n",
    "        variables['momentum_x'] = raw[:, :, :, 0]\n",
    "        variables['momentum_y'] = raw[:, :, :, 1] \n",
    "        variables['momentum_z'] = raw[:, :, :, 2]\n",
    "        \n",
    "        # Magnetic field components\n",
    "        variables['B_x'] = raw[:, :, :, 3]\n",
    "        variables['B_y'] = raw[:, :, :, 4]\n",
    "        variables['B_z'] = raw[:, :, :, 5]\n",
    "    \n",
    "    if nvar >= 7:\n",
    "        # Density\n",
    "        variables['density'] = raw[:, :, :, 6]\n",
    "        \n",
    "        # Calculate velocity from momentum and density\n",
    "        if nvar >= 6:\n",
    "            # Avoid division by zero\n",
    "            rho = np.maximum(variables['density'], 1e-10)\n",
    "            variables['velocity_x'] = variables['momentum_x'] / rho\n",
    "            variables['velocity_y'] = variables['momentum_y'] / rho\n",
    "            variables['velocity_z'] = variables['momentum_z'] / rho\n",
    "            \n",
    "            # Calculate velocity magnitude\n",
    "            variables['velocity_magnitude'] = np.sqrt(\n",
    "                variables['velocity_x']**2 + \n",
    "                variables['velocity_y']**2 + \n",
    "                variables['velocity_z']**2\n",
    "            )\n",
    "            \n",
    "            # Calculate magnetic field magnitude\n",
    "            variables['B_magnitude'] = np.sqrt(\n",
    "                variables['B_x']**2 + \n",
    "                variables['B_y']**2 + \n",
    "                variables['B_z']**2\n",
    "            )\n",
    "    \n",
    "    if nvar >= 8:\n",
    "        # Entropy or internal energy density\n",
    "        variables['entropy'] = raw[:, :, :, 7]\n",
    "        \n",
    "        # Calculate pressure (assuming ideal gas)\n",
    "        if 'density' in variables:\n",
    "            # For ideal gas: P = (Œ≥-1) * internal_energy_density\n",
    "            # Assuming Œ≥ = 5/3 for monoatomic gas\n",
    "            gamma = 5.0/3.0\n",
    "            variables['pressure'] = (gamma - 1) * variables['entropy']\n",
    "    \n",
    "    return variables\n",
    "\n",
    "print(\"üîß FISH-DS data reading functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d73938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun Visualization Helper Functions\n",
    "\n",
    "def create_3d_grid_coordinates(nx: int, ny: int, nz: int, dx: float = 1.0) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Create 3D coordinate grids for visualization.\"\"\"\n",
    "    x = np.arange(nx) * dx\n",
    "    y = np.arange(ny) * dx\n",
    "    z = np.arange(nz) * dx\n",
    "    \n",
    "    # Create 3D coordinate arrays\n",
    "    Z, Y, X = np.meshgrid(z, y, x, indexing='ij')\n",
    "    return X, Y, Z\n",
    "\n",
    "def normalize_scalar_field(field: np.ndarray, min_percentile: float = 1, max_percentile: float = 99) -> np.ndarray:\n",
    "    \"\"\"Normalize scalar field to 0-1 range using percentiles to handle outliers.\"\"\"\n",
    "    vmin = np.percentile(field, min_percentile)\n",
    "    vmax = np.percentile(field, max_percentile)\n",
    "    \n",
    "    if vmax == vmin:\n",
    "        return np.zeros_like(field)\n",
    "    \n",
    "    normalized = (field - vmin) / (vmax - vmin)\n",
    "    return np.clip(normalized, 0, 1)\n",
    "\n",
    "def apply_colormap(scalar_field: np.ndarray, colormap_name: str = 'viridis') -> np.ndarray:\n",
    "    \"\"\"Apply colormap to scalar field and return RGBA values.\"\"\"\n",
    "    cmap = get_cmap(colormap_name)\n",
    "    normalized = normalize_scalar_field(scalar_field)\n",
    "    colors = cmap(normalized)\n",
    "    \n",
    "    # Convert to uint8 RGBA format expected by Rerun\n",
    "    rgba = (colors * 255).astype(np.uint8)\n",
    "    return rgba\n",
    "\n",
    "def subsample_vector_field(vx: np.ndarray, vy: np.ndarray, vz: np.ndarray, \n",
    "                          X: np.ndarray, Y: np.ndarray, Z: np.ndarray,\n",
    "                          reduction_factor: int = 4) -> Tuple[np.ndarray, ...]:\n",
    "    \"\"\"Subsample vector field for visualization performance.\"\"\"\n",
    "    # Subsample the arrays\n",
    "    vx_sub = vx[::reduction_factor, ::reduction_factor, ::reduction_factor]\n",
    "    vy_sub = vy[::reduction_factor, ::reduction_factor, ::reduction_factor]\n",
    "    vz_sub = vz[::reduction_factor, ::reduction_factor, ::reduction_factor]\n",
    "    \n",
    "    X_sub = X[::reduction_factor, ::reduction_factor, ::reduction_factor]\n",
    "    Y_sub = Y[::reduction_factor, ::reduction_factor, ::reduction_factor]\n",
    "    Z_sub = Z[::reduction_factor, ::reduction_factor, ::reduction_factor]\n",
    "    \n",
    "    return vx_sub, vy_sub, vz_sub, X_sub, Y_sub, Z_sub\n",
    "\n",
    "def log_scalar_field_to_rerun(entity_path: str, scalar_field: np.ndarray, \n",
    "                             X: np.ndarray, Y: np.ndarray, Z: np.ndarray,\n",
    "                             colormap: str = 'viridis', alpha_threshold: float = 0.1):\n",
    "    \"\"\"Log a 3D scalar field to Rerun as a point cloud with colors.\"\"\"\n",
    "    \n",
    "    # Flatten arrays\n",
    "    positions = np.stack([X.flatten(), Y.flatten(), Z.flatten()], axis=1)\n",
    "    values = scalar_field.flatten()\n",
    "    \n",
    "    # Apply colormap\n",
    "    colors = apply_colormap(values, colormap)\n",
    "    \n",
    "    # Filter points based on alpha threshold (remove low-value points for performance)\n",
    "    normalized_values = normalize_scalar_field(values)\n",
    "    mask = normalized_values > alpha_threshold\n",
    "    \n",
    "    if np.any(mask):\n",
    "        positions_filtered = positions[mask]\n",
    "        colors_filtered = colors[mask]\n",
    "        \n",
    "        # Log to Rerun\n",
    "        rr.log(\n",
    "            entity_path,\n",
    "            rr.Points3D(\n",
    "                positions=positions_filtered,\n",
    "                colors=colors_filtered[:, :3],  # RGB only\n",
    "                radii=0.5  # Adjust point size as needed\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  No points above threshold for {entity_path}\")\n",
    "\n",
    "def log_vector_field_to_rerun(entity_path: str, vx: np.ndarray, vy: np.ndarray, vz: np.ndarray,\n",
    "                             X: np.ndarray, Y: np.ndarray, Z: np.ndarray,\n",
    "                             scale_factor: float = 1.0, color: Tuple[int, int, int] = (255, 255, 255),\n",
    "                             uniform_length: bool = True, colormap: str = 'viridis'):\n",
    "    \"\"\"Log a 3D vector field to Rerun as arrows with optional uniform length and magnitude-based coloring.\"\"\"\n",
    "    \n",
    "    # Subsample for performance\n",
    "    vx_sub, vy_sub, vz_sub, X_sub, Y_sub, Z_sub = subsample_vector_field(\n",
    "        vx, vy, vz, X, Y, Z, reduction_factor=4\n",
    "    )\n",
    "    \n",
    "    # Create arrow origins and vectors\n",
    "    origins = np.stack([X_sub.flatten(), Y_sub.flatten(), Z_sub.flatten()], axis=1)\n",
    "    vectors = np.stack([vx_sub.flatten(), vy_sub.flatten(), vz_sub.flatten()], axis=1)\n",
    "    \n",
    "    # Calculate magnitudes\n",
    "    magnitudes = np.linalg.norm(vectors, axis=1)\n",
    "    \n",
    "    # Filter out zero vectors\n",
    "    mask = magnitudes > 1e-10\n",
    "    \n",
    "    if np.any(mask):\n",
    "        origins_filtered = origins[mask]\n",
    "        vectors_filtered = vectors[mask]\n",
    "        magnitudes_filtered = magnitudes[mask]\n",
    "        \n",
    "        if uniform_length:\n",
    "            # Normalize vectors to unit length and scale\n",
    "            unit_vectors = vectors_filtered / magnitudes_filtered[:, np.newaxis]\n",
    "            display_vectors = unit_vectors * scale_factor\n",
    "            \n",
    "            # Apply colormap based on magnitude\n",
    "            magnitude_colors = apply_colormap(magnitudes_filtered, colormap)\n",
    "            arrow_colors = magnitude_colors[:, :3]  # RGB only\n",
    "        else:\n",
    "            # Use original scaled vectors\n",
    "            display_vectors = vectors_filtered * scale_factor\n",
    "            # Use uniform color\n",
    "            arrow_colors = np.tile(color, (len(vectors_filtered), 1))\n",
    "        \n",
    "        # Log to Rerun\n",
    "        rr.log(\n",
    "            entity_path,\n",
    "            rr.Arrows3D(\n",
    "                origins=origins_filtered,\n",
    "                vectors=display_vectors,\n",
    "                colors=arrow_colors\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  No non-zero vectors for {entity_path}\")\n",
    "\n",
    "print(\"üé® Rerun visualization helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5188a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main FISH-DS Rerun Visualizer Class\n",
    "\n",
    "class FishDSRerunVisualizer:\n",
    "    \"\"\"\n",
    "    Interactive 3D visualizer for FISH-DS simulation data using Rerun.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_directory: str = \"./data\", lazy_load: bool = True):\n",
    "        \"\"\"Initialize the visualizer with data directory.\n",
    "        \n",
    "        Args:\n",
    "            data_directory: Path to data directory containing proc*/*.dat files\n",
    "            lazy_load: If True, only discover files without loading them immediately\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_directory)\n",
    "        self.dat_files = []\n",
    "        self.current_data = None\n",
    "        self.time_steps = []\n",
    "        self._files_discovered = False\n",
    "        \n",
    "        # Visualization parameters\n",
    "        self.show_density = True\n",
    "        self.show_velocity = True\n",
    "        self.show_magnetic = True\n",
    "        self.show_pressure = False\n",
    "        self.vector_scale = 1.0\n",
    "        self.alpha_threshold = 0.1\n",
    "        \n",
    "        # Color schemes\n",
    "        self.density_colormap = 'viridis'\n",
    "        self.velocity_colormap = 'plasma'\n",
    "        self.magnetic_colormap = 'coolwarm'\n",
    "        self.pressure_colormap = 'hot'\n",
    "        \n",
    "        # Discover files based on lazy_load setting\n",
    "        if not lazy_load:\n",
    "            self.discover_data_files()\n",
    "        else:\n",
    "            print(f\"üèÉ‚Äç‚ôÇÔ∏è Fast initialization enabled. Call viz.discover_data_files() to scan for data files.\")\n",
    "    \n",
    "    def discover_data_files(self):\n",
    "        \"\"\"Find all available .dat files in the data directory.\"\"\"\n",
    "        if self._files_discovered:\n",
    "            print(\"üìÅ Files already discovered. Use refresh=True to rescan.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"üîç Searching for data files in: {self.data_dir.absolute()}\")\n",
    "        \n",
    "        # Check if data directory exists\n",
    "        if not self.data_dir.exists():\n",
    "            # Try alternative paths\n",
    "            alt_paths = [Path(\"../data\"), Path(\"./fish-basic/data\"), Path(\".\")]\n",
    "            for alt_path in alt_paths:\n",
    "                if alt_path.exists():\n",
    "                    self.data_dir = alt_path\n",
    "                    print(f\"  üìÅ Found data directory: {self.data_dir.absolute()}\")\n",
    "                    break\n",
    "            else:\n",
    "                print(\"‚ùå Data directory not found!\")\n",
    "                return\n",
    "        \n",
    "        # Find .dat files in processor subdirectories - optimized scanning\n",
    "        self.dat_files = []\n",
    "        total_files = 0\n",
    "        \n",
    "        try:\n",
    "            # Use glob pattern for faster file discovery\n",
    "            dat_pattern = self.data_dir / \"proc*\" / \"*.dat\"\n",
    "            all_dat_files = list(self.data_dir.glob(\"proc*//*.dat\"))\n",
    "            \n",
    "            if not all_dat_files:\n",
    "                print(\"‚ùå No .dat files found in processor directories\")\n",
    "                return\n",
    "            \n",
    "            # Group by processor directory for reporting\n",
    "            proc_files = {}\n",
    "            for file_path in all_dat_files:\n",
    "                proc_name = file_path.parent.name\n",
    "                if proc_name not in proc_files:\n",
    "                    proc_files[proc_name] = []\n",
    "                proc_files[proc_name].append(file_path)\n",
    "            \n",
    "            # Report findings and collect files\n",
    "            for proc_name, files in sorted(proc_files.items()):\n",
    "                self.dat_files.extend(files)\n",
    "                total_files += len(files)\n",
    "                print(f\"  üìÑ Found {len(files)} files in {proc_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error scanning for files: {e}\")\n",
    "            return\n",
    "        \n",
    "        # Sort files for consistent ordering\n",
    "        self.dat_files.sort()\n",
    "        \n",
    "        # Extract time steps from filenames - optimized parsing\n",
    "        self.time_steps = []\n",
    "        for f in self.dat_files:\n",
    "            try:\n",
    "                # Optimized filename parsing\n",
    "                stem_parts = f.stem.split('_')\n",
    "                if len(stem_parts) >= 2 and stem_parts[1].isdigit():\n",
    "                    time_step = int(stem_parts[1])\n",
    "                    self.time_steps.append(time_step)\n",
    "                else:\n",
    "                    self.time_steps.append(0)\n",
    "            except (ValueError, IndexError):\n",
    "                self.time_steps.append(0)\n",
    "        \n",
    "        print(f\"üìä Total files found: {total_files}\")\n",
    "        if self.dat_files:\n",
    "            print(f\"‚è∞ Time step range: {min(self.time_steps)} - {max(self.time_steps)}\")\n",
    "            self._files_discovered = True\n",
    "        else:\n",
    "            print(\"‚ùå No data files discovered\")\n",
    "    \n",
    "    def refresh_file_list(self):\n",
    "        \"\"\"Force refresh of the file list.\"\"\"\n",
    "        self._files_discovered = False\n",
    "        self.dat_files = []\n",
    "        self.time_steps = []\n",
    "        self.discover_data_files()\n",
    "    \n",
    "    def load_timestep(self, timestep_index: int = 0) -> bool:\n",
    "        \"\"\"Load data for a specific timestep.\"\"\"\n",
    "        # Ensure files are discovered\n",
    "        if not self._files_discovered:\n",
    "            self.discover_data_files()\n",
    "            \n",
    "        if not self.dat_files or timestep_index >= len(self.dat_files):\n",
    "            print(f\"‚ùå Invalid timestep index: {timestep_index} (available: 0-{len(self.dat_files)-1})\")\n",
    "            return False\n",
    "        \n",
    "        filename = self.dat_files[timestep_index]\n",
    "        print(f\"üìñ Loading: {filename.name}\")\n",
    "        \n",
    "        try:\n",
    "            self.current_data = read_fish_data(str(filename))\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {filename}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def visualize_segment(self, segment_index: int = 0, timestep_index: int = 0):\n",
    "        \"\"\"Visualize a specific data segment.\"\"\"\n",
    "        if not self.load_timestep(timestep_index):\n",
    "            return\n",
    "        \n",
    "        if segment_index >= len(self.current_data['segments']):\n",
    "            print(f\"‚ùå Invalid segment index: {segment_index} (available: 0-{len(self.current_data['segments'])-1})\")\n",
    "            return\n",
    "        \n",
    "        segment = self.current_data['segments'][segment_index]\n",
    "        variables = parse_mhd_variables(segment)\n",
    "        \n",
    "        if not variables:\n",
    "            print(\"‚ùå No variables found in segment\")\n",
    "            return\n",
    "        \n",
    "        # Get dimensions and create coordinate grids\n",
    "        nz, ny, nx = segment['dimensions']\n",
    "        dx = self.current_data['dx']\n",
    "        \n",
    "        print(f\"üéØ Visualizing segment {segment_index + 1}: {nx}√ó{ny}√ó{nz}\")\n",
    "        \n",
    "        X, Y, Z = create_3d_grid_coordinates(nx, ny, nz, dx)\n",
    "        \n",
    "        # Set timeline\n",
    "        time_ns = int(self.current_data['time'] * 1e9)  # Convert to nanoseconds\n",
    "        rr.set_time_nanos(\"simulation_time\", time_ns)\n",
    "        \n",
    "        # Log coordinate system\n",
    "        rr.log(\"world\", rr.ViewCoordinates.RIGHT_HAND_Y_UP, static=True)\n",
    "        \n",
    "        # Visualize scalar fields\n",
    "        if self.show_density and 'density' in variables:\n",
    "            log_scalar_field_to_rerun(\n",
    "                \"world/density\",\n",
    "                variables['density'], X, Y, Z,\n",
    "                colormap=self.density_colormap,\n",
    "                alpha_threshold=self.alpha_threshold\n",
    "            )\n",
    "        \n",
    "        if self.show_pressure and 'pressure' in variables:\n",
    "            log_scalar_field_to_rerun(\n",
    "                \"world/pressure\",\n",
    "                variables['pressure'], X, Y, Z,\n",
    "                colormap=self.pressure_colormap,\n",
    "                alpha_threshold=self.alpha_threshold\n",
    "            )\n",
    "        \n",
    "        # Visualize velocity field\n",
    "        if self.show_velocity and all(f'velocity_{c}' in variables for c in ['x', 'y', 'z']):\n",
    "            log_vector_field_to_rerun(\n",
    "                \"world/velocity_field\",\n",
    "                variables['velocity_x'], variables['velocity_y'], variables['velocity_z'],\n",
    "                X, Y, Z,\n",
    "                scale_factor=self.vector_scale,\n",
    "                uniform_length=True,\n",
    "                colormap='plasma'  # Hot colors for velocity magnitude\n",
    "            )\n",
    "        \n",
    "        # Visualize magnetic field\n",
    "        if self.show_magnetic and all(f'B_{c}' in variables for c in ['x', 'y', 'z']):\n",
    "            log_vector_field_to_rerun(\n",
    "                \"world/magnetic_field\",\n",
    "                variables['B_x'], variables['B_y'], variables['B_z'],\n",
    "                X, Y, Z,\n",
    "                scale_factor=self.vector_scale * 0.7,  # Slightly smaller than velocity\n",
    "                uniform_length=True,\n",
    "                colormap='coolwarm'  # Blue-red for magnetic field magnitude\n",
    "            )\n",
    "        \n",
    "        print(f\"‚úÖ Visualization complete for timestep {timestep_index}\")\n",
    "    \n",
    "    def visualize_domain_overview(self, timestep_index: int = 0):\n",
    "        \"\"\"Visualize the domain overview data if available.\"\"\"\n",
    "        if not self.load_timestep(timestep_index):\n",
    "            return\n",
    "        \n",
    "        if not self.current_data['domain_overview']:\n",
    "            print(\"‚ùå No domain overview data available\")\n",
    "            return\n",
    "        \n",
    "        dov = self.current_data['domain_overview']\n",
    "        variables = parse_mhd_variables(dov)\n",
    "        \n",
    "        if not variables:\n",
    "            print(\"‚ùå No variables found in domain overview\")\n",
    "            return\n",
    "        \n",
    "        # Get dimensions and create coordinate grids\n",
    "        nz, ny, nx = dov['dimensions']\n",
    "        dx = self.current_data['dx'] * dov['reduction_factor']\n",
    "        \n",
    "        print(f\"üåê Visualizing domain overview: {nx}√ó{ny}√ó{nz}\")\n",
    "        \n",
    "        X, Y, Z = create_3d_grid_coordinates(nx, ny, nz, dx)\n",
    "        \n",
    "        # Set timeline\n",
    "        time_ns = int(self.current_data['time'] * 1e9)\n",
    "        rr.set_time_nanos(\"simulation_time\", time_ns)\n",
    "        \n",
    "        # Log coordinate system\n",
    "        rr.log(\"world\", rr.ViewCoordinates.RIGHT_HAND_Y_UP, static=True)\n",
    "        \n",
    "        # Visualize overview data (same as segment but with different path)\n",
    "        if self.show_density and 'density' in variables:\n",
    "            log_scalar_field_to_rerun(\n",
    "                \"world/domain_overview/density\",\n",
    "                variables['density'], X, Y, Z,\n",
    "                colormap=self.density_colormap,\n",
    "                alpha_threshold=self.alpha_threshold * 0.5  # Lower threshold for overview\n",
    "            )\n",
    "        \n",
    "        if self.show_velocity and all(f'velocity_{c}' in variables for c in ['x', 'y', 'z']):\n",
    "            log_vector_field_to_rerun(\n",
    "                \"world/domain_overview/velocity_field\",\n",
    "                variables['velocity_x'], variables['velocity_y'], variables['velocity_z'],\n",
    "                X, Y, Z,\n",
    "                scale_factor=self.vector_scale,\n",
    "                uniform_length=True,\n",
    "                colormap='plasma'\n",
    "            )\n",
    "        \n",
    "        if self.show_magnetic and all(f'B_{c}' in variables for c in ['x', 'y', 'z']):\n",
    "            log_vector_field_to_rerun(\n",
    "                \"world/domain_overview/magnetic_field\",\n",
    "                variables['B_x'], variables['B_y'], variables['B_z'],\n",
    "                X, Y, Z,\n",
    "                scale_factor=self.vector_scale * 0.7,\n",
    "                uniform_length=True,\n",
    "                colormap='coolwarm'\n",
    "            )\n",
    "        \n",
    "        print(f\"‚úÖ Domain overview visualization complete for timestep {timestep_index}\")\n",
    "    \n",
    "    def animate_time_series(self, max_timesteps: int = 10, delay: float = 0.5):\n",
    "        \"\"\"Create an animation through multiple timesteps.\"\"\"\n",
    "        # Ensure files are discovered\n",
    "        if not self._files_discovered:\n",
    "            self.discover_data_files()\n",
    "            \n",
    "        if not self.dat_files:\n",
    "            print(\"‚ùå No data files available for animation\")\n",
    "            return\n",
    "            \n",
    "        print(f\"üé¨ Creating animation for {min(max_timesteps, len(self.dat_files))} timesteps\")\n",
    "        \n",
    "        for i in range(min(max_timesteps, len(self.dat_files))):\n",
    "            print(f\"\\n‚èØÔ∏è  Frame {i+1}/{min(max_timesteps, len(self.dat_files))}\")\n",
    "            \n",
    "            # Try domain overview first, fall back to first segment\n",
    "            if self.current_data and self.current_data.get('domain_overview'):\n",
    "                self.visualize_domain_overview(i)\n",
    "            else:\n",
    "                self.visualize_segment(0, i)\n",
    "            \n",
    "            time.sleep(delay)\n",
    "        \n",
    "        print(\"\\nüéâ Animation complete!\")\n",
    "    \n",
    "    def configure_visualization(self, show_density: bool = True, show_velocity: bool = True,\n",
    "                             show_magnetic: bool = True, show_pressure: bool = False,\n",
    "                             vector_scale: float = 1.0, alpha_threshold: float = 0.1):\n",
    "        \"\"\"Configure visualization parameters.\"\"\"\n",
    "        self.show_density = show_density\n",
    "        self.show_velocity = show_velocity\n",
    "        self.show_magnetic = show_magnetic\n",
    "        self.show_pressure = show_pressure\n",
    "        self.vector_scale = vector_scale\n",
    "        self.alpha_threshold = alpha_threshold\n",
    "        \n",
    "        print(\"‚öôÔ∏è  Visualization configuration updated:\")\n",
    "        print(f\"   üìä Density: {show_density}\")\n",
    "        print(f\"   üåä Velocity: {show_velocity}\")\n",
    "        print(f\"   üß≤ Magnetic: {show_magnetic}\")\n",
    "        print(f\"   üå°Ô∏è  Pressure: {show_pressure}\")\n",
    "        print(f\"   üìè Vector scale: {vector_scale}\")\n",
    "        print(f\"   üëÅÔ∏è  Alpha threshold: {alpha_threshold}\")\n",
    "    \n",
    "    def get_file_info(self):\n",
    "        \"\"\"Get information about discovered files.\"\"\"\n",
    "        if not self._files_discovered:\n",
    "            print(\"üìÅ Files not yet discovered. Call discover_data_files() first.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"üìä File Information:\")\n",
    "        print(f\"   Total files: {len(self.dat_files)}\")\n",
    "        print(f\"   Time steps: {min(self.time_steps)} to {max(self.time_steps)}\")\n",
    "        print(f\"   Data directory: {self.data_dir.absolute()}\")\n",
    "        \n",
    "        if len(self.dat_files) > 0:\n",
    "            print(f\"   First file: {self.dat_files[0].name}\")\n",
    "            print(f\"   Last file: {self.dat_files[-1].name}\")\n",
    "\n",
    "print(\"üéÆ FishDSRerunVisualizer class defined successfully!\")\n",
    "print(\"üí° Performance improvements:\")\n",
    "print(\"   - Lazy loading: Files are only scanned when needed\")\n",
    "print(\"   - Optimized file discovery using glob patterns\")\n",
    "print(\"   - Better error handling and user feedback\")\n",
    "print(\"   - Fast initialization option\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a005d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FISH-DS Rerun Visualizer (Optimized)\n",
    "\n",
    "# Create the visualizer instance with fast initialization\n",
    "print(\"üöÄ Creating FISH-DS Rerun Visualizer...\")\n",
    "viz = FishDSRerunVisualizer(data_directory=\"./data\", lazy_load=True)\n",
    "\n",
    "# Discover data files (this step can be skipped for even faster startup)\n",
    "print(\"\\nüîç Discovering data files...\")\n",
    "viz.discover_data_files()\n",
    "\n",
    "# Check if data was found\n",
    "if not viz.dat_files:\n",
    "    print(\"\\n‚ùå No data files found! Please ensure you have FISH-DS output files in:\")\n",
    "    print(\"   ./data/proc*/*.dat\")\n",
    "    print(\"\\nüí° If your data is elsewhere, try:\")\n",
    "    print(\"   viz = FishDSRerunVisualizer('path/to/your/data')\")\n",
    "    print(\"   viz.discover_data_files()\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Successfully initialized with {len(viz.dat_files)} data files\")\n",
    "    print(f\"üìÖ Time steps available: {min(viz.time_steps)} to {max(viz.time_steps)}\")\n",
    "    \n",
    "    # Configure visualization settings\n",
    "    viz.configure_visualization(\n",
    "        show_density=True,      # Show density as colored points\n",
    "        show_velocity=True,     # Show velocity vectors (green arrows)\n",
    "        show_magnetic=True,     # Show magnetic field vectors (red arrows)\n",
    "        show_pressure=False,    # Don't show pressure by default\n",
    "        vector_scale=2.0,       # Scale factor for vector arrows\n",
    "        alpha_threshold=0.2     # Only show points above this normalized value\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüéØ Ready for visualization!\")\n",
    "    print(\"\\nüìã Quick start commands:\")\n",
    "    print(\"   viz.visualize_segment(0, 0)                           # First segment, first timestep\")\n",
    "    print(\"   viz.visualize_domain_overview(0)                      # Domain overview, first timestep\")\n",
    "    print(\"   viz.animate_time_series(max_timesteps=3, delay=1.0)   # Short animation\")\n",
    "    print(\"\\nüîß Utility commands:\")\n",
    "    print(\"   viz.get_file_info()                                   # Show file information\")\n",
    "    print(\"   viz.refresh_file_list()                               # Rescan for new files\")\n",
    "    print(\"   viz.configure_visualization(...)                      # Change settings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad71d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = FishDSRerunVisualizer(data_directory=\"./data\", lazy_load=True)\n",
    "viz.configure_visualization(vector_scale=10)\n",
    "viz.animate_time_series(100, delay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d80ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizer with enhanced vector settings\n",
    "viz_enhanced = FishDSRerunVisualizer(data_directory=\"./data\", lazy_load=True)\n",
    "\n",
    "# Configure for optimal vector visualization\n",
    "viz_enhanced.configure_visualization(\n",
    "    show_density=True,        # Show density as background\n",
    "    show_velocity=True,       # Show velocity vectors\n",
    "    show_magnetic=True,       # Show magnetic field vectors\n",
    "    show_pressure=False,      # Skip pressure for cleaner view\n",
    "    vector_scale=1.5,         # Good arrow length\n",
    "    alpha_threshold=0       # Only show significant density points\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
